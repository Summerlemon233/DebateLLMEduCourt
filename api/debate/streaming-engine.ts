import { LLMFactory, LLMProvider } from '../llm/factory';
import { DebateStage, DebateResult, ModelConfig, LLMResponse, DebateUpdateEvent } from '../../src/types';
import { DebateError } from '../utils/error-handler';

export interface DebateRequest {
  question: string;
  models: string[];
  config?: ModelConfig;
}

export type StreamCallback = (event: DebateUpdateEvent) => void;

export class StreamingDebateEngine {
  private llmFactory: LLMFactory;

  constructor(llmFactory: LLMFactory) {
    this.llmFactory = llmFactory;
  }

  async runStreamingDebate(
    request: DebateRequest, 
    onUpdate: StreamCallback
  ): Promise<void> {
    const startTime = Date.now();
    
    try {
      // éªŒè¯è¾“å…¥
      this.validateRequest(request);
      await this.validateModels(request.models);

      const stages: DebateStage[] = [];

      // é˜¶æ®µ1ï¼šåˆå§‹å›ç­”
      const stage1 = await this.runStreamingStage1(request, onUpdate);
      stages.push(stage1);
      onUpdate({
        type: 'stage_complete',
        stageNumber: 1,
        stage: stage1
      });

      // é˜¶æ®µ2ï¼šäº¤å‰è´¨ç–‘ä¸å®Œå–„
      const stage2 = await this.runStreamingStage2(request, stage1, onUpdate);
      stages.push(stage2);
      onUpdate({
        type: 'stage_complete',
        stageNumber: 2,
        stage: stage2
      });

      // é˜¶æ®µ3ï¼šæœ€ç»ˆéªŒè¯
      const stage3 = await this.runStreamingStage3(request, stage1, stage2, onUpdate);
      stages.push(stage3);
      onUpdate({
        type: 'stage_complete',
        stageNumber: 3,
        stage: stage3
      });

      // ç”Ÿæˆæ€»ç»“
      const summary = await this.generateSummary(request, stages);
      
      // å‘é€å®Œæˆäº‹ä»¶
      onUpdate({
        type: 'debate_complete',
        stageNumber: 3,
        isComplete: true,
        summary
      });

    } catch (error) {
      console.error('Streaming debate execution failed:', error);
      throw error;
    }
  }

  private validateRequest(request: DebateRequest): void {
    if (!request.question || request.question.trim().length === 0) {
      throw new DebateError('VALIDATION_ERROR', 'Question is required and cannot be empty', undefined, 400);
    }

    if (!request.models || request.models.length === 0) {
      throw new DebateError('VALIDATION_ERROR', 'At least one model must be selected', undefined, 400);
    }

    if (request.models.length > 6) {
      throw new DebateError('VALIDATION_ERROR', 'Maximum 6 models allowed', undefined, 400);
    }

    if (request.question.length > 1000) {
      throw new DebateError('VALIDATION_ERROR', 'Question too long (max 1000 characters)', undefined, 400);
    }
  }

  private async validateModels(models: string[]): Promise<void> {
    const availableProviders = this.llmFactory.getAvailableProviders();
    const unavailableModels = models.filter(model => !availableProviders.includes(model as any));
    
    if (unavailableModels.length > 0) {
      throw new DebateError(
        'MODEL_UNAVAILABLE',
        `Models not available: ${unavailableModels.join(', ')}`,
        { unavailableModels, availableModels: availableProviders },
        400
      );
    }
  }

  private async runStreamingStage1(
    request: DebateRequest, 
    onUpdate: StreamCallback
  ): Promise<DebateStage> {
    const stage: DebateStage = {
      stage: 1,
      title: 'åˆå§‹è§‚ç‚¹',
      description: 'å„æ¨¡å‹æä¾›å¯¹é—®é¢˜çš„åˆå§‹è§‚ç‚¹å’Œåˆ†æ',
      responses: [],
      startTime: new Date().toISOString(),
      endTime: '',
      duration: 0
    };

    const startTime = Date.now();

    // å‘é€é˜¶æ®µå¼€å§‹äº‹ä»¶
    onUpdate({
      type: 'stage_start',
      stageNumber: 1
    });

    // ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆåˆå§‹å›ç­”çš„æç¤ºè¯
    const prompt = this.generateStage1Prompt(request.question);

    // é¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å‹ä»¥å®ç°å®æ—¶æ›´æ–°
    for (const model of request.models) {
      try {
        console.log(`ğŸ”„ æ­£åœ¨å¤„ç†æ¨¡å‹: ${model}`);
        const client = this.llmFactory.getClient(model as any);
        const response = await client.generateResponse(prompt, request.config);
        
        console.log(`âœ… æ¨¡å‹ ${model} å“åº”å®Œæˆï¼Œç«‹å³å‘é€äº‹ä»¶`);
        // ç«‹å³å‘é€æ¨¡å‹å“åº”äº‹ä»¶
        onUpdate({
          type: 'model_response',
          stageNumber: 1,
          model,
          response
        });

        stage.responses.push(response);
      } catch (error) {
        console.error(`Model ${model} failed in stage 1:`, error);
        const errorResponse: LLMResponse = {
          model,
          content: `é”™è¯¯ï¼š${error instanceof Error ? error.message : String(error)}`,
          timestamp: new Date().toISOString(),
          usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
          responseTime: 0
        };

        onUpdate({
          type: 'model_response',
          stageNumber: 1,
          model,
          response: errorResponse
        });

        stage.responses.push(errorResponse);
      }
    }

    stage.endTime = new Date().toISOString();
    stage.duration = Date.now() - startTime;

    return stage;
  }

  private async runStreamingStage2(
    request: DebateRequest, 
    stage1: DebateStage,
    onUpdate: StreamCallback
  ): Promise<DebateStage> {
    const stage: DebateStage = {
      stage: 2,
      title: 'äº¤å‰è´¨ç–‘ä¸å®Œå–„',
      description: 'å„æ¨¡å‹å¯¹å…¶ä»–æ¨¡å‹çš„è§‚ç‚¹è¿›è¡Œè´¨ç–‘ã€è¡¥å……å’Œå®Œå–„',
      responses: [],
      startTime: new Date().toISOString(),
      endTime: '',
      duration: 0
    };

    const startTime = Date.now();

    // å‘é€é˜¶æ®µå¼€å§‹äº‹ä»¶
    onUpdate({
      type: 'stage_start',
      stageNumber: 2
    });

    // ç”ŸæˆåŒ…å«å…¶ä»–æ¨¡å‹è§‚ç‚¹çš„æç¤ºè¯
    const prompt = this.generateStage2Prompt(request.question, stage1);

    // é¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å‹
    for (const model of request.models) {
      try {
        console.log(`ğŸ”„ é˜¶æ®µ2å¤„ç†æ¨¡å‹: ${model}`);
        const client = this.llmFactory.getClient(model as any);
        const response = await client.generateResponse(prompt, request.config);
        
        console.log(`âœ… é˜¶æ®µ2æ¨¡å‹ ${model} å“åº”å®Œæˆ`);
        onUpdate({
          type: 'model_response',
          stageNumber: 2,
          model,
          response
        });

        stage.responses.push(response);
      } catch (error) {
        console.error(`Model ${model} failed in stage 2:`, error);
        const errorResponse: LLMResponse = {
          model,
          content: `é”™è¯¯ï¼š${error instanceof Error ? error.message : String(error)}`,
          timestamp: new Date().toISOString(),
          usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
          responseTime: 0
        };

        onUpdate({
          type: 'model_response',
          stageNumber: 2,
          model,
          response: errorResponse
        });

        stage.responses.push(errorResponse);
      }
    }

    stage.endTime = new Date().toISOString();
    stage.duration = Date.now() - startTime;

    return stage;
  }

  private async runStreamingStage3(
    request: DebateRequest, 
    stage1: DebateStage, 
    stage2: DebateStage,
    onUpdate: StreamCallback
  ): Promise<DebateStage> {
    const stage: DebateStage = {
      stage: 3,
      title: 'æœ€ç»ˆéªŒè¯',
      description: 'å„æ¨¡å‹åŸºäºå‰é¢çš„è®¨è®ºæä¾›æœ€ç»ˆè§‚ç‚¹å’Œç»“è®º',
      responses: [],
      startTime: new Date().toISOString(),
      endTime: '',
      duration: 0
    };

    const startTime = Date.now();

    // å‘é€é˜¶æ®µå¼€å§‹äº‹ä»¶
    onUpdate({
      type: 'stage_start',
      stageNumber: 3
    });

    // ç”ŸæˆåŒ…å«æ‰€æœ‰å‰é¢è®¨è®ºçš„æç¤ºè¯
    const prompt = this.generateStage3Prompt(request.question, stage1, stage2);

    // é¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å‹
    for (const model of request.models) {
      try {
        console.log(`ğŸ”„ é˜¶æ®µ3å¤„ç†æ¨¡å‹: ${model}`);
        const client = this.llmFactory.getClient(model as any);
        const response = await client.generateResponse(prompt, request.config);
        
        console.log(`âœ… é˜¶æ®µ3æ¨¡å‹ ${model} å“åº”å®Œæˆ`);
        onUpdate({
          type: 'model_response',
          stageNumber: 3,
          model,
          response
        });

        stage.responses.push(response);
      } catch (error) {
        console.error(`Model ${model} failed in stage 3:`, error);
        const errorResponse: LLMResponse = {
          model,
          content: `é”™è¯¯ï¼š${error instanceof Error ? error.message : String(error)}`,
          timestamp: new Date().toISOString(),
          usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
          responseTime: 0
        };

        onUpdate({
          type: 'model_response',
          stageNumber: 3,
          model,
          response: errorResponse
        });

        stage.responses.push(errorResponse);
      }
    }

    stage.endTime = new Date().toISOString();
    stage.duration = Date.now() - startTime;

    return stage;
  }

  private generateStage1Prompt(question: string): string {
    return `
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„è¾©è®ºå‚ä¸è€…ï¼Œè¯·å¯¹ä»¥ä¸‹é—®é¢˜æä¾›ä½ çš„åˆå§‹è§‚ç‚¹å’Œåˆ†æï¼š

é—®é¢˜ï¼š${question}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. æ˜ç¡®è¡¨è¾¾ä½ çš„ç«‹åœºå’Œè§‚ç‚¹
2. æä¾›æ”¯æŒä½ è§‚ç‚¹çš„ä¸»è¦è®ºæ®ï¼ˆè‡³å°‘3ä¸ªï¼‰
3. åˆ†æå¯èƒ½çš„åå¯¹è§‚ç‚¹
4. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œè®ºè¯æœ‰åŠ›
5. å›ç­”é•¿åº¦æ§åˆ¶åœ¨300-500å­—
6. æ”¯æŒä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…æ‹¬**ç²—ä½“**ã€*æ–œä½“*ã€åˆ—è¡¨ã€å¼•ç”¨ç­‰æ¥å¢å¼ºå¯è¯»æ€§

è¯·å¼€å§‹ä½ çš„å›ç­”ï¼š
    `.trim();
  }

  private generateStage2Prompt(question: string, stage1: DebateStage): string {
    let othersViews = '';
    stage1.responses.forEach((response, index) => {
      othersViews += `\n\n### è§‚ç‚¹${index + 1}ï¼ˆ${response.model}ï¼‰ï¼š\n${response.content}`;
    });

    return `
ç°åœ¨è¿›å…¥è¾©è®ºçš„ç¬¬äºŒé˜¶æ®µï¼šäº¤å‰è´¨ç–‘ä¸å®Œå–„ã€‚

åŸå§‹é—®é¢˜ï¼š${question}

å…¶ä»–å‚ä¸è€…çš„åˆå§‹è§‚ç‚¹ï¼š${othersViews}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼š
1. æŒ‡å‡ºå…¶ä»–è§‚ç‚¹ä¸­ä½ è®¤ä¸ºæœ‰é—®é¢˜æˆ–ä¸å¤Ÿå……åˆ†çš„åœ°æ–¹
2. å¯¹ä½ è®¤ä¸ºæœ‰ä»·å€¼çš„è§‚ç‚¹ç»™äºˆè®¤å¯å’Œè¡¥å……
3. è¿›ä¸€æ­¥å®Œå–„å’Œå¼ºåŒ–ä½ è‡ªå·±çš„è§‚ç‚¹
4. æå‡ºæ–°çš„è®ºæ®æˆ–è¯æ®æ¥æ”¯æŒä½ çš„ç«‹åœº
5. ä¿æŒå»ºè®¾æ€§å’Œä¸“ä¸šæ€§
6. æ”¯æŒä½¿ç”¨Markdownæ ¼å¼æ¥å¢å¼ºå¯è¯»æ€§

è¯·æä¾›ä½ çš„åˆ†æå’Œå®Œå–„åçš„è§‚ç‚¹ï¼š
    `.trim();
  }

  private generateStage3Prompt(
    question: string, 
    stage1: DebateStage, 
    stage2: DebateStage
  ): string {
    let allDiscussion = '';
    
    allDiscussion += '\n## åˆå§‹è§‚ç‚¹é˜¶æ®µ\n';
    stage1.responses.forEach((response, index) => {
      allDiscussion += `\n### è§‚ç‚¹${index + 1}ï¼ˆ${response.model}ï¼‰ï¼š\n${response.content}`;
    });

    allDiscussion += '\n\n## è´¨ç–‘ä¸å®Œå–„é˜¶æ®µ\n';
    stage2.responses.forEach((response, index) => {
      allDiscussion += `\n### å®Œå–„è§‚ç‚¹${index + 1}ï¼ˆ${response.model}ï¼‰ï¼š\n${response.content}`;
    });

    return `
ç°åœ¨è¿›å…¥è¾©è®ºçš„æœ€ç»ˆé˜¶æ®µï¼šæœ€ç»ˆéªŒè¯ä¸æ€»ç»“ã€‚

åŸå§‹é—®é¢˜ï¼š${question}

å®Œæ•´è®¨è®ºè¿‡ç¨‹ï¼š${allDiscussion}

è¯·åŸºäºæ•´ä¸ªè®¨è®ºè¿‡ç¨‹ï¼š
1. æ€»ç»“ä½ çš„æœ€ç»ˆç«‹åœºå’Œæ ¸å¿ƒè§‚ç‚¹
2. æ•´åˆè®¨è®ºä¸­çš„æœ‰ä»·å€¼è§è§£
3. æŒ‡å‡ºè®¨è®ºä¸­è¾¾æˆçš„å…±è¯†ï¼ˆå¦‚æœæœ‰ï¼‰
4. æ‰¿è®¤ä»å­˜åœ¨åˆ†æ­§çš„åœ°æ–¹
5. æä¾›ä½ çš„æœ€ç»ˆç»“è®ºå’Œå»ºè®®
6. æ”¯æŒä½¿ç”¨Markdownæ ¼å¼æ¥å¢å¼ºå¯è¯»æ€§

è¯·æä¾›ä½ çš„æœ€ç»ˆè§‚ç‚¹ï¼š
    `.trim();
  }

  private async generateSummary(
    request: DebateRequest, 
    stages: DebateStage[]
  ): Promise<string> {
    // ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹æ¥ç”Ÿæˆæ€»ç»“
    const summarizer = this.llmFactory.getClient(request.models[0] as any);
    
    let debateContent = `é—®é¢˜ï¼š${request.question}\n\n`;
    
    stages.forEach(stage => {
      debateContent += `## ${stage.title}\n`;
      stage.responses.forEach((response, index) => {
        debateContent += `\n### ${response.model}çš„è§‚ç‚¹ï¼š\n${response.content}\n`;
      });
      debateContent += '\n';
    });

    const summaryPrompt = `
è¯·ä¸ºä»¥ä¸‹è¾©è®ºè¿‡ç¨‹ç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§æ€»ç»“ï¼š

${debateContent}

è¯·æä¾›ï¼š
1. é—®é¢˜çš„æ ¸å¿ƒäº‰è®®ç‚¹
2. å„æ–¹ä¸»è¦è§‚ç‚¹çš„æ¢³ç†
3. è®¨è®ºä¸­çš„å…±è¯†ä¸åˆ†æ­§
4. æœ‰ä»·å€¼çš„è§è§£å’Œè®ºæ®
5. æ€»ä½“ç»“è®ºå’Œæ€è€ƒ

æ€»ç»“åº”è¯¥å®¢è§‚ã€å…¨é¢ï¼Œé•¿åº¦æ§åˆ¶åœ¨200-300å­—ï¼Œæ”¯æŒä½¿ç”¨Markdownæ ¼å¼ï¼š
    `.trim();

    try {
      const summaryResponse = await summarizer.generateResponse(summaryPrompt);
      return summaryResponse.content;
    } catch (error) {
      console.error('Failed to generate summary:', error);
      return 'æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹å…·ä½“çš„è¾©è®ºå†…å®¹ã€‚';
    }
  }
}
